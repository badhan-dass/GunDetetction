{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79628168-369c-462f-a874-ef1b3e1e7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from moviepy.editor import *\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Input, RepeatVector, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.layers import LSTM, GRU, Bidirectional, Attention\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7e8fd-b73e-457d-bc9f-5d3d6e010c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1c225-6070-4719-a440-3804bf2069d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7679e01-65a0-4e65-9752-3e07a5644de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e0fed-d033-4bc1-b123-7b7ba1ab0c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the list to a file\n",
    "# with open('file_paths_test.json', 'w') as file:\n",
    "#    json.dump(file_paths_test, file)\n",
    "\n",
    "# Load the list from the file\n",
    "with open('file_paths_test.json', 'r') as file:\n",
    "    loaded_list = json.load(file)\n",
    "\n",
    "print(loaded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d2aa1-1ed8-4774-956a-425f3196de4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "mobileNetGRU = load_model('VGG+GRU_best_weights.keras')\n",
    "mobileNetGRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca3635-6401-4a5a-a6ed-b52c14e2fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(video_file_path, SEQUENCE_LENGTH):\n",
    " \n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    " \n",
    "    # Get the width and height of the video.\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    " \n",
    "    # Declare a list to store video frames we will extract.\n",
    "    frames_list = []\n",
    "    \n",
    "    # Store the predicted class in the video.\n",
    "    predicted_class_name = ''\n",
    " \n",
    "    # Get the number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    " \n",
    "    # Calculate the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n",
    " \n",
    "    # Iterating the number of times equal to the fixed length of sequence.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    " \n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    " \n",
    "        success, frame = video_reader.read() \n",
    " \n",
    "        if not success:\n",
    "            break\n",
    " \n",
    "        # Resize the Frame to fixed Dimensions.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Normalize the resized frame.\n",
    "        normalized_frame = resized_frame / 255\n",
    "        \n",
    "        # Appending the pre-processed frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    " \n",
    "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
    "    predicted_labels_probabilities = mobileNetGRU.predict(np.expand_dims(frames_list, axis = 0))[0]\n",
    " \n",
    "    # Get the index of class with highest probability.\n",
    "    predicted_label = np.argmax(predicted_labels_probabilities)\n",
    " \n",
    "    # Get the class name using the retrieved index.\n",
    "    predicted_class_name = class_categories_list[predicted_label]\n",
    "    \n",
    "    # Display the predicted class along with the prediction confidence.\n",
    "    print(f'Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n",
    "    return predicted_class_name, predicted_labels_probabilities[predicted_label]\n",
    "    video_reader.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285541a5-13fd-4969-8bff-6cefe46d2d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"TestSet/\"\n",
    "\n",
    "# List to store .mp4 file paths\n",
    "loaded_list_new = []\n",
    "\n",
    "# List files in the given folder\n",
    "for file in os.listdir(folder_path):\n",
    "    # Check if the file ends with .mp4 and is a file (not a directory)\n",
    "    if file.endswith(\".mp4\") and os.path.isfile(os.path.join(folder_path, file)):\n",
    "        loaded_list_new.append(os.path.join(folder_path, file))\n",
    "\n",
    "print(\"Len of the test set:\", len(loaded_list_new))\n",
    "print(loaded_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c55d31-87bf-450c-947c-dd545c26100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list of .mp4 file paths\n",
    "# Array to store 1s and 0s\n",
    "import numpy as np\n",
    "ground_truth_array = []\n",
    "\n",
    "# Search for the string \"No_Gun\" in each element\n",
    "for file_path in loaded_list_new:\n",
    "    if \"No_Gun\" in file_path:\n",
    "        ground_truth_array.append(1)\n",
    "    else:\n",
    "        ground_truth_array.append(0)\n",
    "\n",
    "ground_truth_array=np.array(ground_truth_array)\n",
    "print(type(ground_truth_array))\n",
    "ground_truth_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6a0da-d0f9-4f25-8048-0e6cbc820251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155e3b6-4dff-4142-aed3-bf7e5c720fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81879f5c-cb19-4be1-bebb-776859eae031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Perform Single Prediction on the Test Video.\n",
    "SEQUENCE_LENGTH = 30\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 224\n",
    "class_categories_list = [\"Gun\", \"NoGun\"]  # Update to match the corrected mapping\n",
    "GunCount = 0\n",
    "NoGunCount = 0\n",
    "\n",
    "# Lists to save paths for TP, FP, FN\n",
    "true_positives = []  # Correctly predicted \"Gun\"\n",
    "false_positives = []  # Predicted \"Gun\" but actually \"NoGun\"\n",
    "false_negatives = []  # Predicted \"NoGun\" but actually \"Gun\"\n",
    "true_negatives = []  # Correctly predicted \"NoGun\"\n",
    "\n",
    "time_per_video=[]\n",
    "for ii in range(len(loaded_list_new)):\n",
    "    start_time = time.time()\n",
    "    predicted_class_name, predicted_labels_probabilities = predict_video(loaded_list_new[ii], SEQUENCE_LENGTH)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    time_per_video.append(execution_time)\n",
    "    # Get GPU memory usage in MB\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "    else:\n",
    "        gpu_memory_usage = \"N/A\"\n",
    "\n",
    "    print(f\"Video {ii+1}: {loaded_list_new[ii]}\")\n",
    "    print(\"Prediction: \", predicted_class_name, \"Confidence: \", predicted_labels_probabilities)\n",
    "    print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
    "    print(f\"GPU Memory Usage: {gpu_memory_usage:.2f} MB\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get ground truth label for current video\n",
    "    actual_class_name = class_categories_list[ground_truth_array[ii]]\n",
    "\n",
    "    if predicted_class_name == \"Gun\":\n",
    "        if actual_class_name == \"Gun\":  # True Positive\n",
    "            true_positives.append(loaded_list_new[ii])\n",
    "        else:  # False Positive\n",
    "            false_positives.append(loaded_list_new[ii])\n",
    "    else:\n",
    "        if actual_class_name == \"Gun\":  # False Negative\n",
    "            false_negatives.append(loaded_list_new[ii])\n",
    "        else:  # True Negative\n",
    "            true_negatives.append(loaded_list_new[ii])\n",
    "            \n",
    "\n",
    "# Print summary\n",
    "print(f\"Total Gun Videos: {GunCount}, Total NoGun Videos: {NoGunCount}\")\n",
    "print(\"True Positives (TP):\", len(true_positives))\n",
    "print(\"False Positives (FP):\", len(false_positives))\n",
    "print(\"False Negatives (FN):\", len(false_negatives))\n",
    "print(\"True Negatives (TN):\", len(true_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a093cf6-9133-486a-9052-f83cf2800011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(time_per_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f4b22-a966-44dc-95cd-00c6417a57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "145.46+38.5284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5ffce-f869-4688-b1c4-991acb6dfa15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(true_positives), len(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07a51d-10b1-4460-a715-26e76df64295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62594a8d-640c-4e38-82ae-516522137f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate IoU\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    x_min1, y_min1, w1, h1 = bbox1\n",
    "    x_min2, y_min2, w2, h2 = bbox2\n",
    "\n",
    "    # Calculate the intersection\n",
    "    x_intersect = max(x_min1, x_min2)\n",
    "    y_intersect = max(y_min1, y_min2)\n",
    "    x_intersect2 = min(x_min1 + w1, x_min2 + w2)\n",
    "    y_intersect2 = min(y_min1 + h1, y_min2 + h2)\n",
    "\n",
    "    if x_intersect >= x_intersect2 or y_intersect >= y_intersect2:\n",
    "        return 0  # No intersection\n",
    "\n",
    "    intersection_area = (x_intersect2 - x_intersect) * (y_intersect2 - y_intersect)\n",
    "    area_bbox1 = w1 * h1\n",
    "    area_bbox2 = w2 * h2\n",
    "    union_area = area_bbox1 + area_bbox2 - intersection_area\n",
    "\n",
    "    return intersection_area / union_area\n",
    "\n",
    "# Function to match predictions and ground truths for evaluation\n",
    "def match_predictions(predictions, annotations, iou_threshold=0.5):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    scores = []\n",
    "    IoU = []\n",
    "\n",
    "    for frame_id, pred_bboxes in predictions.items():\n",
    "        gt_bboxes = [anno['bbox'] for anno in annotations if anno['image_id'] == frame_id]\n",
    "        gt_used = set()\n",
    "\n",
    "        if gt_bboxes:\n",
    "            # Ground truth bounding boxes are present\n",
    "            for pred in pred_bboxes:\n",
    "                matched = False\n",
    "                for idx, gt_bbox in enumerate(gt_bboxes):\n",
    "                    if idx in gt_used:\n",
    "                        continue\n",
    "                    iou = calculate_iou(pred[:4], gt_bbox)\n",
    "\n",
    "                    if iou >= iou_threshold:\n",
    "                        y_true.append(1)  # True positive\n",
    "                        y_pred.append(1)\n",
    "                        scores.append(pred[4])  # Confidence score\n",
    "                        IoU.append(iou)\n",
    "                        gt_used.add(idx)\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "                if not matched:\n",
    "                    y_true.append(0)  # False positive\n",
    "                    y_pred.append(1)\n",
    "                    scores.append(pred[4])  # Confidence score\n",
    "\n",
    "            # Add unmatched ground truths as false negatives\n",
    "            y_true.extend([1] * (len(gt_bboxes) - len(gt_used)))\n",
    "            y_pred.extend([0] * (len(gt_bboxes) - len(gt_used)))\n",
    "            scores.extend([0] * (len(gt_bboxes) - len(gt_used)))\n",
    "        else:\n",
    "            # No ground truth bounding boxes present\n",
    "            if pred_bboxes:  # If predictions are made without ground truth\n",
    "                for pred in pred_bboxes:\n",
    "                    y_true.append(0)  # False positive\n",
    "                    y_pred.append(1)\n",
    "                    scores.append(pred[4])  # Confidence score\n",
    "\n",
    "    return y_true, y_pred, scores, np.mean(IoU) if IoU else 0\n",
    "\n",
    "# Path to the YOLO model\n",
    "model_path = os.path.join('.', 'runs', 'detect', 'train3', 'weights', 'best.pt')\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Directory containing labels\n",
    "label_dir = \"TestSet/\"\n",
    "\n",
    "# List of video files\n",
    "videos_list1 = true_positives\n",
    "\n",
    "videos_list2 = false_negatives\n",
    "\n",
    "# Combine video lists\n",
    "videos = videos_list2 + videos_list1 \n",
    "\n",
    "# Get the list of JSON files\n",
    "labels = {os.path.splitext(f)[0].replace('_label', ''): f for f in os.listdir(label_dir) if f.endswith('_label.json')}\n",
    "\n",
    "# Initialize global metrics\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "all_scores = []\n",
    "all_ious = []\n",
    "averaged_metrics = {\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": [],\n",
    "    \"Average Precision (AP)\": [],\n",
    "    \"IoU\": []\n",
    "}\n",
    "\n",
    "time_and_storage_metrics = {}\n",
    "\n",
    "for counter, video_file in enumerate(videos, start=1):\n",
    "    video_path = video_file\n",
    "    video_name = os.path.splitext(os.path.basename(video_file))[0]\n",
    "\n",
    "    # Identify matching annotation file based on naming pattern\n",
    "    if video_file in videos_list2:\n",
    "        label_file = None  # No annotation file for videos without the desired object\n",
    "    elif video_name.endswith('_video'):\n",
    "        label_file = labels.get(video_name.replace('_video', ''))  # Match video with corresponding annotation file\n",
    "    else:\n",
    "        label_file = None  # No annotation file for videos without the desired object\n",
    "\n",
    "    # Start memory tracking\n",
    "    process = psutil.Process(os.getpid())\n",
    "    initial_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "\n",
    "    # Initialize frame-level time tracking\n",
    "    total_time_taken = 0  # Initialize total inference time\n",
    "    \n",
    "    # Predict on the video\n",
    "    predictions = {}\n",
    "    for idx, result in enumerate(model.predict(source=video_path, show=False, save=False, conf=0.5, line_width=2, show_labels=True, show_conf=True, classes=[0], stream=True)):\n",
    "        frame_time_ms = result.speed[\"inference\"]  # Extract inference time specifically\n",
    "        total_time_taken += frame_time_ms\n",
    "    \n",
    "        frame_id = idx\n",
    "        frame_preds = []\n",
    "    \n",
    "        for pred in result.boxes.data:\n",
    "            x_min, y_min, x_max, y_max, conf, class_id = pred\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "            frame_preds.append([x_min.item(), y_min.item(), width.item(), height.item(), conf.item()])\n",
    "    \n",
    "        predictions[frame_id] = frame_preds\n",
    "\n",
    "\n",
    "    # Calculate memory used\n",
    "    final_memory = process.memory_info().rss / (1024 ** 2)  # Memory in MB\n",
    "    memory_used = final_memory - initial_memory\n",
    "\n",
    "    # Store time and storage metrics for the video\n",
    "    time_and_storage_metrics[video_file] = {\n",
    "        \"Total Time Taken (ms)\": total_time_taken\n",
    "        #\"Memory Used (MB)\": memory_used\n",
    "    }\n",
    "\n",
    "    # Load ground truth annotations\n",
    "    annotations = []\n",
    "    if label_file:\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "        with open(label_path) as f:\n",
    "            gt_data = json.load(f)\n",
    "            annotations = gt_data.get('annotations', [])  # Default to empty if no annotations exist\n",
    "\n",
    "    # Match predictions with ground truths and calculate metrics\n",
    "    y_true, y_pred, scores, iou = match_predictions(predictions, annotations)\n",
    "\n",
    "    # Calculate individual video metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    average_precision = average_precision_score(y_true, scores) if scores else 0.0\n",
    "\n",
    "    video_metrics = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Average Precision (AP)\": average_precision,\n",
    "        \"IoU\": iou\n",
    "    }\n",
    "\n",
    "    # Accumulate global metrics\n",
    "    all_y_true.extend(y_true)\n",
    "    all_y_pred.extend(y_pred)\n",
    "    all_scores.extend(scores)\n",
    "    all_ious.append(iou)\n",
    "\n",
    "    # Collect metrics for averaging\n",
    "    for key in averaged_metrics:\n",
    "        averaged_metrics[key].append(video_metrics[key])\n",
    "\n",
    "    print(f\"Counter: {counter}\")    \n",
    "    print(f\"Processing Video: {video_file}, Annotation File: {label_file if label_file else 'No Annotations'}\")\n",
    "    print(f\"Metrics for Video {video_file}: {video_metrics}\")\n",
    "    print(f\"Total Time Taken: {total_time_taken:.2f} milliseconds\")\n",
    "    print(f\"Memory Used: {memory_used:.2f} MB\")\n",
    "    print(f\"Ground_Truth length: {len(all_y_true)}, Prediction length: {len(all_y_pred)}\")\n",
    "    print(f\"Ground_Truth: {all_y_true}\")\n",
    "    print(f\"Predictions: {all_y_pred}\")\n",
    "\n",
    "# Calculate holistic metrics\n",
    "holistic_precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "holistic_recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "holistic_f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "holistic_average_precision = average_precision_score(all_y_true, all_scores) if all_scores else 0.0\n",
    "\n",
    "holistic_metrics = {\n",
    "    \"Precision\": holistic_precision,\n",
    "    \"Recall\": holistic_recall,\n",
    "    \"F1 Score\": holistic_f1,\n",
    "    \"Average Precision (AP)\": holistic_average_precision,\n",
    "    \"Mean Average Precision (mAP)\": holistic_average_precision,\n",
    "    \"IoU\": np.mean(all_ious) if all_ious else 0,\n",
    "\n",
    "}\n",
    "\n",
    "# Calculate simple averaged metrics\n",
    "averaged_metrics_result = {key: np.mean(values) for key, values in averaged_metrics.items()}\n",
    "\n",
    "\n",
    "print(\"\\nHolistic Metrics:\")\n",
    "print(holistic_metrics)\n",
    "\n",
    "print(\"\\nAveraged Metrics Across Videos:\")\n",
    "print(averaged_metrics_result)\n",
    "\n",
    "\n",
    "print(\"\\nTime and Storage Metrics for Each Video:\")\n",
    "for video, metrics in time_and_storage_metrics.items():\n",
    "    print(f\"{video}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894cfcb-d034-4c97-a4db-7f4f74779a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016f105-c131-49f8-8f7d-4590472b37a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nHolistic Metrics:\")\n",
    "print(holistic_metrics)\n",
    "\n",
    "\n",
    "print(\"\\nTime and Storage Metrics for Each Video:\")\n",
    "for video, metrics in time_and_storage_metrics.items():\n",
    "    print(f\"{video}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182ae4c-c033-4475-b39d-e4408f440a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['Total Time Taken (ms)'])\n",
    "\n",
    "timesGun = []\n",
    "timesNoGun = []\n",
    "for video, metrics in time_and_storage_metrics.items():\n",
    "    if \"No_Gun_video\" in video:\n",
    "        timesNoGun.append(metrics['Total Time Taken (ms)'])\n",
    "    else:\n",
    "        timesGun.append(metrics['Total Time Taken (ms)'])\n",
    "\n",
    "print(len(timesGun), len(timesNoGun))\n",
    "print(sum(timesGun)/1000, sum(timesNoGun)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7181d-172e-41fc-8b22-7d217135ed30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33543a-7200-452c-be1e-519d88c8d350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937aed5-860b-4d48-8894-6d756110751c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef883f96-6563-48d7-98c9-5936b399e3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91b42b-5f1d-41ab-848a-6fca7ecdb26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8092f-eb77-46f9-8a71-cb0d63355fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228284d6-45ce-400e-a600-ad547d4f49f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
